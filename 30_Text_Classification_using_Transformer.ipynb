{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOI08aVyF1ny"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.datasets import imdb\n",
        "from keras.utils import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        # embed_dim: This parameter specifies the dimensionality of the input and output embeddings.\n",
        "        # num_heads: This parameter controls the number of attention heads in the multi-head attention layer.\n",
        "        # ff_dim: This parameter specifies the dimensionality of the feedforward network.\n",
        "        # rate: This parameter controls the dropout rate, which is used to regularize the network and prevent overfitting.\n",
        "\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        # This creates a MultiHeadAttention layer, responsible for learning long-range dependencies within sequences.\n",
        "        self.ffn = Sequential(\n",
        "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim),]\n",
        "        )\n",
        "        # self.ffn: This creates a feedforward network, often used for additional nonlinear transformations.\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        # self.layernorm1 and self.layernorm2: These create LayerNormalization layers, which help stabilize training and improve convergence.\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "        # self.dropout1 and self.dropout2: These create Dropout layers, randomly setting a portion of inputs to zero during training to prevent overfitting.\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        # Applies multi-head attention to the input sequence, allowing different parts of the sequence to interact with each other.\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        # Applies dropout to the attention output.\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        # Adds the attention output to the original input and applies layer normalization. This residual connection helps prevent vanishing gradients and improves training stability.\n",
        "        ffn_output = self.ffn(out1)\n",
        "        # Passes the normalized output through the feedforward network.\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        # Applies dropout to the feedforward output.\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "        # Adds the feedforward output to the previous layer's output and applies final layer normalization before returning the result."
      ],
      "metadata": {
        "id": "KTH_GCwZGLBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        # maxlen: The maximum length of the input sequences the model will handle.\n",
        "        # vocab_size: The total number of unique tokens (words) in the vocabulary.\n",
        "        # embed_dim: The dimensionality of the embeddings (how each token and its position will be represented as a vector).\n",
        "        super().__init__()\n",
        "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        # An Embedding layer that maps each token in the input sequence\n",
        "        # to a dense vector of size embed_dim.\n",
        "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "        # An Embedding layer that maps each position in the sequence\n",
        "        # (from 0 to maxlen-1) to a dense vector of size embed_dim.\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        # Extracts the actual length of the current input sequence.\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        # Creates a tensor of positions from 0 to maxlen-1.\n",
        "        positions = self.pos_emb(positions)\n",
        "        # Looks up the position embeddings for each position in the sequence.\n",
        "        x = self.token_emb(x)\n",
        "        # Looks up the token embeddings for each token in the input sequence.\n",
        "        return x + positions\n",
        "        # Adds the token embeddings and position embeddings element-wise,\n",
        "        # resulting in a combined representation that captures both word\n",
        "        # meaning and positional information."
      ],
      "metadata": {
        "id": "Zmy2qKmALndm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000  # Only consider the top 20k words\n",
        "maxlen = 200  # Only consider the first 200 words of each movie review\n",
        "(x_train, y_train), (x_val, y_val) = imdb.load_data(num_words=vocab_size)\n",
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val = pad_sequences(x_val, maxlen=maxlen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHxMPITEPB6u",
        "outputId": "ac0b36f4-3766-4a03-e742-b289013ddf6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "25000 Training sequences\n",
            "25000 Validation sequences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J-3awqqPs_I",
        "outputId": "0977464c-175f-4130-e717-72f8beb3b326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 200)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dropout(0.1)(x)\n",
        "x = Dense(20, activation=\"relu\")(x)\n",
        "x = Dropout(0.1)(x)\n",
        "outputs = Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "dPDURSEdQoiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "history = model.fit(\n",
        "    x_train, y_train, batch_size=32,\n",
        "    epochs=10,\n",
        "    validation_data=(x_val, y_val)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2MK6OnldJFg",
        "outputId": "76b97424-002c-48fe-c994-e6013327ef64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 136s 169ms/step - loss: 0.3919 - accuracy: 0.8105 - val_loss: 0.2901 - val_accuracy: 0.8776\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 120s 153ms/step - loss: 0.1991 - accuracy: 0.9232 - val_loss: 0.3143 - val_accuracy: 0.8732\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 128s 164ms/step - loss: 0.1304 - accuracy: 0.9537 - val_loss: 0.3935 - val_accuracy: 0.8636\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 129s 165ms/step - loss: 0.0834 - accuracy: 0.9721 - val_loss: 0.5215 - val_accuracy: 0.8535\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 120s 153ms/step - loss: 0.0562 - accuracy: 0.9826 - val_loss: 0.6220 - val_accuracy: 0.8472\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 130s 166ms/step - loss: 0.0364 - accuracy: 0.9896 - val_loss: 0.6928 - val_accuracy: 0.8443\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 129s 165ms/step - loss: 0.0338 - accuracy: 0.9901 - val_loss: 0.7609 - val_accuracy: 0.8405\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 130s 167ms/step - loss: 0.0256 - accuracy: 0.9925 - val_loss: 0.7541 - val_accuracy: 0.8402\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 130s 166ms/step - loss: 0.0217 - accuracy: 0.9936 - val_loss: 0.8622 - val_accuracy: 0.8346\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 131s 168ms/step - loss: 0.0178 - accuracy: 0.9950 - val_loss: 0.9473 - val_accuracy: 0.8310\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train"
      ],
      "metadata": {
        "id": "xqJafibgflFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61585822-fc18-4520-9a1d-b4e88f3da565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   5,   25,  100, ...,   19,  178,   32],\n",
              "       [   0,    0,    0, ...,   16,  145,   95],\n",
              "       [   0,    0,    0, ...,    7,  129,  113],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,    4, 3586,    2],\n",
              "       [   0,    0,    0, ...,   12,    9,   23],\n",
              "       [   0,    0,    0, ...,  204,  131,    9]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "49eKdxTSeLmA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}